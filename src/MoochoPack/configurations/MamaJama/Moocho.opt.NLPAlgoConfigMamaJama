********************************************************************
*** All of these options can be used with the rSQPAlgo_ConfigMamaJama
*** algorithm configuration class.  This file will be maintained and
*** will include every option that users can set.  Most of these
*** options the user will want to leave alone but they are there
*** in any case.

begin_options

**********************************************************
*** Options specific for the rSQP algorithm configuration
*** class rSQPAlgo_ConfigMamaJama.
***
options_group rSQPAlgo_ConfigMamaJama {

*** Algorithmic/Implementation Options

*** Variable Reduction range/null space decomposition

*    null_space_matrix = AUTO;         *** Let the solver decide (default)
*    null_space_matrix = EXPLICIT;     *** Store D = -inv(C)*N explicitly
*    null_space_matrix = IMPLICIT;     *** Perform operations implicity with C, N

*    range_space_matrix = AUTO;        *** Let the algorithm decide dynamically (default)
*    range_space_matrix = COORDINATE;  *** Y = [ I; 0 ] (Cheaper computationally)
*    range_space_matrix = ORTHOGONAL;  *** Y = [ I; -N'*inv(C') ] (more stable)

*    max_basis_cond_change_frac = -1.0;  *** (default)
     *** (+-dbl) If < 0 then the solver will decide what value to use.
     *** Otherwise this is the (see printed algorithm description) above
     *** which a new basis will be selected.  Example values:
     ***    -1 : Allow solver to decide (default) 
     ***     0 : Switch to a new basis every iteration (not a good idea)
     ***   100 : Seitch to a new basis when change is more that 100?
     *** 1e+50 : (big number) Never switch to a new basis.

*** Reduced Hessian Approximations

*    exact_reduced_hessian = true; *** Use NLP Hessian info if available
*    exact_reduced_hessian = false; *** Use quasi_newton (default)
     *** If true and if the NLP supports second order information
     *** (hessian of the lagrangian HL) then the exact reduced hessian
     *** rHL = Z'*HL*Z will be computed at each iteration.

*    quasi_newton = AUTO;   *** Let solver decide dynamically (default)
*    quasi_newton = BFGS;   *** Dense BFGS
*    quasi_newton = LBFGS;  *** Limited memory BFGS
     *** [exact_reduced_hessian == false]
     *** ToDo: Finish documentation!

*    max_dof_quasi_newton_dense = -1; *** (default)
     *** [quasi_newton == AUTO] (+-int) If < 0 let solver decide when to switch
     *** to limited memory LBFGS quasi-newton updating.  Otherwise LBFGS
     *** will be used rather than dense BFGS when the number of degrees of
     *** freedom n-m is larger than thhis number.  Example values:
     ***  -1 : (< 0) Let the solver decide dynamically (default)
     ***   0 : Always use limited memory LBFGS.
     *** 500 : Use LBFG when n-m >= 500 and dense BFGS when n-m < 500.
     *** 

*    num_lbfgs_updates_stored   = -1; *** (default)
     *** [quasi_newton == L*BFGS] (+-int) If < 0 then let solver decide
     *** otherwise this is the maximum number of update vectors stored
     *** for limite memory LBFGS.

*    lbfgs_auto_scaling = true;  *** (defalut)
*    lbfgs_auto_scaling = false;
     *** [quasi_newton == LBFGS] If true then auto scaling of initial
     *** hessian approximation will be use for LBFGS.

*** Line search methods

*    line_search_method = AUTO;               *** Let the solver decide dynamically (default)
*    line_search_method = NONE;               *** Take full steps at every iteration
*    line_search_method = DIRECT;             *** Use standard Armijo backtracking
*    line_search_method = 2ND_ORDER_CORRECT;  *** Like DIRECT except computes corrections for
*                                             *** c(x) before backtracking line search
*    line_search_method = WATCHDOG;           *** Like DIRECT except uses watchdog type trial steps
     *** Options:
     *** AUTO : Let the solver decide dynamically what line search method to use (if any)
     *** NONE : Take full steps at every iteration.  For most problems this is a bad idea.
     ***    However, for some problems this can help when starting close to the solution usually.
     *** DIRECT : Use a standard Armijo backtracking line search at every iteration.
     *** 2ND_ORDER_CORRECT : Like DIRECT except computes corrections for before applying
     ***     the backtracking line search (see options_group LineSearch2ndOrderCorrect).
     ***     This can help greatly on some problems and can counter act the Maritos effect.

*    merit_function_type = AUTO;              *** [line_search_method != NONE] Let solver decide
*    merit_function_type = L1;                *** [line_search_method != NONE] phi(x) = f(x) + mu*||c(x)||1
*    merit_function_type = MODIFIED_L1;       *** [line_search_method != NONE] phi(x) = f(x) + sum{mu(j),|cj(x)|,j}
*    merit_function_type = MODIFIED_L1_INCR;  *** [line_search_method != NONE] Like MODIFIED_L1 except mu(j) are
*                                             *** altered in order to take larger steps
*    l1_penalty_parameter_update = AUTO;      *** [merit_function_type == L1] let solver decide
*    l1_penalty_parameter_update = WITH_MULT; *** [merit_function_type == L1] Use Lagrange multipliers to update mu
*    l1_penalty_parameter_update = MULT_FREE; *** [merit_function_type == L1] Don't use Lagrange multipliers to update mu

}

***************************************************************
*** Options for EvalNewPoint for NLPFirstOrderInfo.
*** See options_group NLPFirstDerivativesTester
***
options_group EvalNewPointStd {

*    fd_deriv_testing = FD_DEFAULT; *** (default) Test if check_results==true (see above)
*    fd_deriv_testing = FD_TEST;    *** Always test
*    fd_deriv_testing = FD_NO_TEST; *** never test
     *** Determines if the derivatives of the NLP returned from the NLPFirstOrderInfo interface
     *** are correct using finite differences (see the options_group NLPFirstDerivativesTester).
     *** Valid options include:
     ***    FD_DEFAULT : Perform the finite difference tests if check_results==true.
     ***    FD_TEST    : Always test, regardless of the value of check_results.
     ***    FD_NO_TEST : Never test, regardless of the value of check_results.

*    decomp_sys_testing = DST_DEFAULT; *** (default) Test if check_results==true (see above)
*    decomp_sys_testing = DST_TEST;    *** Always test
*    decomp_sys_testing = DST_NO_TEST; *** never test
     *** Determines if the range/null decomposition matrices from DecompositionSystem are
     *** tested or not (see the options_group DecompositionSystemTester).
     *** Valid options include:
     ***    FD_DEFAULT : Perform the tests if check_results==true.
     ***    FD_TEST    : Always test, regardless of the value of check_results.
     ***    FD_NO_TEST : Never test, regardless of the value of check_results.

*    decomp_sys_testing_print_level = DSPL_USE_GLOBAL;    *** (default) Use the value in journal_print_level (see above).
*    decomp_sys_testing_print_level = DSPL_LEAVE_DEFAULT; *** Leave whatever setting in already in use.
     *** This option allows the user to determine how the testing print level is determined.
     *** Valid options include:
     ***    DSLP_USE_GLOBAL    : The value of journal_print_level will be used to determine reasonable values for
     ***                         print_tests and dump_all.
     ***    DSLP_LEAVE_DEFAULT : Whatever values are currently set for DecompositionSystemTester::print_tests and
     ***                         DecompositionSystemTester::dump_all will be used (see the options_group
     ***                         DecompositionSystemTester).

}

***************************************************************
*** Options for determining if variable bounds
*** xL <= x <= xU are violated by
*** more than an acceptable tolerance.
***
options_group VariableBoundsTester {
*    warning_tol   = 1e-10;
*    error_tol     = 1e-5; *** The default is max_var_bounds_viol above.
}

***********************************************************
*** Options for the finite derivative testing for a
*** standard NLP.
***
options_group NLPFirstDerivativesTester {
*    fd_testing_method = FD_COMPUTE_ALL; *** Compute all of the derivatives (O(m))
*    fd_testing_method = FD_DIRECTIONAL; *** Only compute along random directions (O(1))
*    num_fd_directions = 1;  *** [fd_testing_method == DIRECTIONAL]
*    warning_tol   = 1e-10;
*    warning_tol   = 0.0;  *** Show me all comparisons.
*    error_tol     = 1e-5;
}

***************************************************************
*** Options for EvalNewPoint for a "Tailored Approach" NLP.
*** See options_group NLPrSQPTailoredApproachTester
***
options_group EvalNewPointTailoredApproach {
*    fd_deriv_testing   = FD_DEFAULT;  *** Test if check_results==true (see above)
*    fd_deriv_testing   = FD_TEST;    *** Always test
*    fd_deriv_testing   = FD_NO_TEST; *** never test
}

****************************************************************
*** Options for the finite difference derivative tester for a 
*** direct sensistivity NLP.
***
options_group NLPFirstOrderDirectTester {
*    Gf_testing_method = FD_COMPUTE_ALL; *** Compute all of the derivatives (O(n))
*    Gf_testing_method = FD_DIRECTIONAL; *** Only compute along random directions (O(1))
*    Gf_warning_tol   = 1e-10;
*    Gf_error_tol     = 1e-5;
*    Gc_testing_method = FD_COMPUTE_ALL; *** Compute all of the derivatives (O(n-m))
*    Gc_testing_method = FD_DIRECTIONAL; *** Only compute along random directions
*    Gc_warning_tol   = 1e-10;
*    Gc_error_tol     = 1e-5;
*    num_fd_directions = 1;  *** [testing_method == DIRECTIONAL]
}

****************************************************************
*** Options for the BasisSystem tester used to validate the
*** basis of the constraints Jacobian.
***
options_group BasisSystemTester {
*    num_random_vectors = 1;      *** (+int) Number of sets of random vectors to use
*    mult_warning_tol   = 1e-15;  *** (+dbl) Warning tolerance for checking matrix-vector multiplication
*    mult_error_tol     = 1e-12;  *** (+dbl) Error tolerance for checking matrix-vector multiplication
*    solve_warning_tol  = 1e-12;  *** (+dbl) Warning tolerance for checking linear solves
*    solve_error_tol    = 1e-10;  *** (+dbl) Error tolerance for checking linear solves
}

****************************************************************
*** Options for the DecompositionSystem tester used to validate
*** range/null decomposition matrices (NLPFirstOrderInfo only).
***
options_group DecompositionSystemTester {
*    print_tests = PRINT_NONE;    *** (default)
*    print_tests = PRINT_BASIC;
*    print_tests = PRINT_MORE;
*    print_tests = PRINT_ALL;
*    dump_all = true;
*    dump_all = false;            *** (default)
*    num_random_vectors = 1;      *** (+int) Number of sets of random vectors to use
*    mult_warning_tol   = 1e-15;  *** (+dbl) Warning tolerance for checking matrix-vector multiplication
*    mult_error_tol     = 1e-12;  *** (+dbl) Error tolerance for checking matrix-vector multiplication
*    solve_warning_tol  = 1e-12;  *** (+dbl) Warning tolerance for checking linear solves
*    solve_error_tol    = 1e-10;  *** (+dbl) Error tolerance for checking linear solves
}

*********************************************************************
*** Options for finite difference initialization of reduced hessian.
*** 
*** [rSQPAlgo_ConfigMamaJama::hessian_initialization == FINITE_DIFF_*]
***
options_group InitFinDiffReducedHessian {
*    initialization_method	= SCALE_IDENTITY;
*    initialization_method	= SCALE_DIAGONA;
*    initialization_method	= SCALE_DIAGONAL_ABS;
*    max_cond			= 1e+1;
*    min_diag			= 1e-8;
*    step_scale			= 1e-1;
}

*********************************************************************
*** Options for the check for if to skip the BFGS update.
***
*** [rSQPAlgo_ConfigMamaJama::exact_hessian == false]
***
options_group CheckSkipBFGSUpdateStd {
*    skip_bfgs_prop_const = 10.0; *** (+dbl)
}

*********************************************************************
*** Options the BFGS updating (dense or limited memory)
*** 
*** [rSQPAlgo_ConfigMamaJama::exact_hessian == false]
***
options_group BFGSUpdate {

*    rescale_init_identity = true;  *** (default)
*    rescale_init_identity = false;
     *** If true, then rescale the initial identity matrix at 2nd iteration

*    use_dampening = true;  *** (default)
*    use_dampening = false;
     *** Use dampened BFGS update

*    secant_testing          = DEFAULT;  *** Test secant condition if check_results==true (see above)
*    secant_testing          = TEST;     *** Always test secant condition
*    secant_testing          = NO_TEST;  *** Never test secant condition

*    secant_warning_tol      = 1e-14;
*    secant_error_tol        = 1e-10;

}

*********************************************************************
*** Options for the convergence test.
***
*** See the printed step description (i.e. 'rSQPppAlgo.out') for a
*** description of what these options do.
***
options_group CheckConvergenceStd {

*    scale_kkt_error_by   = SCALE_BY_NORM_2_X;
*    scale_kkt_error_by   = SCALE_BY_NORM_INF_X;
*    scale_kkt_error_by   = SCALE_BY_ONE;        *** (default)
     *** Determines if all of the error mesures are scaled by the size of the unknowns x_k
     *** SCALE_BY_NORM_2_X   : Scale the optimiality conditions by 1/(1+||x_k||2)
     *** SCALE_BY_NORM_INF_X : Scale the optimality conditions by 1/(1+||x_k||inf)
     *** SCALE_BY_ONE        : Scale the optimality conditions by 1

*    scale_opt_error_by_Gf = true; *** (default)
*    scale_opt_error_by_Gf = false;
     *** Determines if the linear dependence of gradients (i.e. ||rGL_k||inf or ||GL_k||inf)
     *** is scaled by the gradient of the objective function or not.
     *** true  : Scale ||rGL_k||inf ir ||GL_k|| by an additional 1/(1+||Gf||inf)
     *** false : Scale ||rGL_k||inf ir ||GL_k|| by an addiational 1 (i.e. no extra scaling)

}

********************************************************************
*** Options for the direct line search object that is used in all the
*** line search methods for the SQP step.
***
*** [rSQPAlgo_ConfigMamaJama::line_search_method != NONE]
***
options_group DirectLineSearchArmQuadSQPStep {
*    slope_frac       = 1.0e-4;
*    min_frac_step    = 0.1:
*    max_frac_step    = 0.5;
*    max_ls_iter      = 20;
}

*******************************************************************
*** Options for the watchdog line search.
*** 
*** [rSQPAlgo_ConfigMamaJama::line_search_method = WATCHDOG]
***
options_group LineSearchWatchDog {
*    opt_kkt_err_threshold	= 1e-3; *** (+dbl)
*    feas_kkt_err_threshold	= 1e-3; *** (+dbl)
     *** Start the watchdog linesearch when opt_kkt_err_k < opt_kkt_err_threshold and
     *** feas_kkt_err_k < feas_kkt_err_threshold
}

*******************************************************************
*** Options for the second order correction line search.
*** 
*** [rSQPAlgo_ConfigMamaJama::line_search_method == 2ND_ORDER_CORRECT]
***
options_group LineSearch2ndOrderCorrect {

*    newton_olevel = PRINT_USE_DEFAULT;   *** O(?) output (default)
*    newton_olevel = PRINT_NOTHING;       *** No output
*    newton_olevel = PRINT_SUMMARY_INFO;  *** O(max_newton_iter) output
*    newton_olevel = PRINT_STEPS;         *** O(max_newton_iter) output
*    newton_olevel = PRINT_VECTORS;       *** O(max_newton_iter*n) output
     *** Determines the amount of output printed to the journal output stream.
     *** PRINT_USE_DEFAULT: Use the output level from the overall algortihm
     ***     to print comperable output (see journal_output_level).
     *** PRINT_NOTHING: Don't print anything (overriddes default print level).
     *** PRINT_SUMMARY_INFO: Print a nice little summary table showing the sizes of the
     ***    newton steps used to compute a feasibility correction as well as what
     ***    progress is being made.
     *** PRINT_STEPS: Don't print a summary table and instead print some more detail
     ***    as to what computations are being performed etc.
     *** PRINT_VECTORS: Print out relavant vectors as well for the feasibility Newton
     ***     iterations.

*    constr_norm_threshold = 1.0; *** (default)
     *** (+dbl) Tolerance for ||c_k||inf below which a 2nd order correction step
     *** will be considered (see printed description).  Example values:
     ***    0.0: Never consider computing a correction.
     ***   1e-3: Consider a correction if and only if ||c_k||inf <= 1e-3
     ***  1e+50: (big number) Consider a correction regardless of ||c_k||inf.

*    constr_incr_ratio = 10.0; *** (default)
     *** (+dbl) Tolerance for ||c_kp1||inf/(1.0+||c_k||inf) below which a 2nd order
     *** correction step will be considered (see printed description).  Example values:
     ***   0.0: Consider computing a correrction only if ||c_kp1||inf is zero.
     ***   10.0: Consider computing a correction if and only if
     ***        ||c_kp1||inf/(1.0+||c_k||inf) < 10.0.
     *** 1e+50: (big number) Consider a corrrection requardless how big ||c_kp1||inf is.

*    after_k_iter = 0; *** (default)
     *** (+int) Number of SQP iterations before a 2nd order correction will be considered
     *** (see printed description).  Example values:
     ***        0: Consider computing a correction right away at the first iteration.
     ***        2: Consider computing a correction when k >= 2.
     ***   999999: (big number) Never consider a 2nd order correction.

*    forced_constr_reduction = LESS_X_D;
*    forced_constr_reduction = LESS_X; *** (default)
     *** Determine the amount of reduction required for c(x_k+d+w).
     ***   LESS_X_D: phi(c(x_k+d+w)) < forced_reduct_ratio * phi(c(x_k+d)) is all that is required.
     ***             As long as a feasible step can be computed, only one newton
     ***             iteration should be required for this.
     ***   LESS_X:   phi(c(x_k+d+w)) < forced_reduct_ratio * phi(c(x_k)) is required.
     ***             In general, this may require several feasibility step calculations
     ***             and several newton iterations.  Of course the maxinum number of
     ***             newton iterations may be exceeded before this is achieved.

*    forced_reduct_ratio = 1.0; *** (default)
     *** (+dbl) (< 1) Fraction of reduction in phi(c(x)) for required reduction.
     *** Example values:
     ***    0.0: The constraints must be fully converged and newton iterations will
     ***         performed until max_newton_itr is exceeded.
     ***    0.5: Require an extra 50% of the required reduction.
     ***    1.0: Don't require any extra reduction.

*    max_step_ratio = 1.0; *** (default)
     *** (+dbl) Maxumum ratio of ||w^p||inf/||d||inf allowed for correction step w^p before
     *** a line search along phi(c(x_k+d+b*w^p)) is performed.  The purpose of this parameter
     *** is to limit the number of line search iterations needed for each feasibility
     *** step and to keep the full w = sum(w^p,p=1...) from getting too big. Example values:
     ***    0.0: Don't allow any correction step.
     ***    0.1: Allow ||w^p||inf/||d||inf <= 0.1.
     ***    1.0: Allow ||w^p||inf/||d||inf <= 1.0.
     ***  1e+50: (big number) Allow ||w^p||inf/||d||inf to be a big as possible.

*    max_newton_iter = 3; *** (default)
     *** (+int) Limit the number of newton feasibilty iterations (with line searches)
     *** allowed.  Example values:
     ***       0: Don't allow any newton iterations (no 2nd order correction).
     ***       3: Allow 3 newton iterations
     ***  999999: Allow any number of newton iteations (not a good idea)

}

*******************************************************************
*** Options for generating feasibility steps for reduced space.
*** 
*** [rSQPAlgo_ConfigMamaJama::line_search_method == 2ND_ORDER_CORRECT]
***
options_group FeasibilityStepReducedStd {
*    qp_objective = OBJ_MIN_FULL_STEP;
*    qp_objective = OBJ_MIN_NULL_SPACE_STEP;
*    qp_objective = OBJ_RSQP;
*    qp_testing   = QP_TEST_DEFAULT;
*    qp_testing   = QP_TEST;
*    qp_testing   = QP_NO_TEST;
}

******************************************************************
*** Options for the direct line search object for
*** the newton steps of the 2nd order correction (see above).
*** 
*** [rSQPAlgo_ConfigMamaJama::line_search_method == 2ND_ORDER_CORRECT]
***
options_group DirectLineSearchArmQuad2ndOrderCorrectNewton {
*    slope_frac       = 1.0e-4;
*    min_frac_step    = 0.1:
*    max_frac_step    = 0.5;
*    max_ls_iter      = 20;
}

******************************************************************
*** Change how the penalty parameters for the merit function
*** are adjusted.
*** 
*** [rSQPAlgo_ConfigMamaJama::line_search_method != NONE]
***
options_group MeritFuncPenaltyParamUpdate {
*    small_mu     = 1e-6;
*    min_mu_ratio = 1e-8
*    mult_factor  = 7.5e-4;
*    kkt_near_sol = 1e-1;
}

*****************************************************************
*** Change how the penalty parameters for the modifed
*** L1 merit function are increased.
***
*** [rSQPAlgo_ConfigMamaJama::line_search_method != NONE]
*** [rSQPAlgo_ConfigMamaJama::merit_function_type == MODIFIED_L1_INCR]
***
options_group MeritFuncModifiedL1LargerSteps {

*    after_k_iter                  = 3;
     *** (+int) Number of SQP iterations before considering increasing penalties.
     *** Set to 0 to start at the first iteration.

*    obj_increase_threshold        = 1e-4;
     *** (+-dbl) Consider increasing penalty parameters when the relative
     *** increase in the objective function is greater than this value.
     *** Set to a very large negative number (i.e. -1e+100) to always
     *** allow increasing the penalty parameters.

*    max_pos_penalty_increase      = 1.0;
     *** (+dbl) Ratio the multipliers are allowed to be increased.
     *** Set to a very large number (1e+100) to allow increasing the penalties
     *** to any value if it will help in taking a larger step.  This will
     *** in effect put all of the wieght of the constraints and will force
     *** the algorithm to only minimize the infeasibilities and ignore
     *** optimality.

*    pos_to_neg_penalty_increase   = 1.0;  *** (+dbl)

*    incr_mult_factor              = 1e-4; *** (+dbl)

}

end_options

*** End
