/** \mainpage NLPInterfacePack: Interfaces to Nonlinear Programs.

This package is a set of interfaces to Nonlinear Programs (NLPs).  Also 
included are some node classes to aid in the implemenation of these interfaces as
well as some testing classes.  The code in this library is divided into two
separate collections (and compiled into two separate libraries).

<ol>

<li> Fundamental interfaces to Nonlinear Programs (NLPs)
(<tt>\ref libNLPInterfacePack_grp "libNLPInterfacePack"</tt>):
This is a library of interfaces to NLPs and minimal extra support code.  The
base level NLP interface is <tt>\ref NLPInterfacePack::NLP "NLP"</tt>.

<li> Auxiliary tools for building NLP and testing NLP subclasses
(<tt>\ref libNLPInterfacePackTools_grp "libNLPInterfacePackTools"</tt>):
This library contains some helper node implementation classes for constructing
concrete NLP subclasses.  Also, classes for testing NLP interfaces are included.
For every NLP interface, there is corresponding testing code that can be used.

</ol>

ToDo: Finish documentation!

*/
//@{

/** \defgroup libNLPInterfacePack_grp libNLPInterfacePack: Fundamental interfaces to Nonlinear Programs (NLPs).

This library contains the fundamental interfaces to NLPs needed to implement several different types of optimization
algorithms.

The formulation of the NLP used by this set of interfaces is as follows:
\verbatim

     min    f(x)
     s.t.   c(x) = 0
            hl <= h(x) <= hu
	        xl <= x <= xu
	where:
	        x    <: R^n
	        c(x) <: R^n -> R^m 
	        h(x) <: R^n -> R^mI 
\endverbatim
This is a very general form of an NLP and allows an optimization algorithm great flexibility in how
it is dealt with.  For example, an interior-point algorithm may add slack variables \c s to convert the
general inequality constraints <tt>hl <= h(x) <= hu</tt> to <tt>h(x) - s == 0</tt> with <tt>hl <= s <= hu</tt>.
However, information as the linearity, or nonlinearity of the governing functions is not available (as it
is in more explicit interfaces like <A HREF="http://www.ampl.com">AMPL</A>).

What is different about these NLP interfaces is that the problem data (vectors, matrices etc.) are
abstracted behind a set of interfaces (see <A HREF="../../AbstractLinAlgPack/html/index.html">AbstractLinAlgPack</A>)
that allow for great flexibility in terms of data structures and computing environments.  For example, vectors
are abstracted behind interfaces and polymorphich <tt>\ref AbstractLinAlgPack::VectorSpace "VectorSpace"</tt> objects
are used to create the needed vector objects from the various vector spaces (i.e. the space for \a x, the space
for \a c(x) and the space for \a h(x) ).  Using these interfaces (as well as some others defined here ???) it is possible
to write optimization algorithms that are independent of nasty details such
as sparse matrix data structures and parallel computing environments.  This allows the implementation an optimization  
to have a much larger impact that what is currently possible.  This is simply not possible in the current generation
of interfaces to mathematical programs.

The most basic NLP interface is called <tt>\ref NLPInterfacePack::NLP "NLP"</tt> (of all the crazy things).  This base
level interface only includes methods for calculating zero order information for the objective and the constraint
functions (i.e \a f(x), \a c(x) and \a h(x)).  Also included in this base interface is access to the other defining
properties of the NLP such as the basic vector spaces, the bound vectors \c xl, \c xu, \c hl and \c hu and initial guess
for the solution.

In order to implement fast and efficient algorithms for NLPs (e.g. SQP, GRG etc.) derivative information is needed for
the governing objective and constraint functions.  However, derivative information is more readily available in some
application areas than other.  To account for this, access to derivative information is partitioned out in a few different
NLP interfaces in an inheritance hierarchy.

In most applications, the simplest type of derivative information to obtain is the gradient of the objective function
\f$ \nabla f(x) \in \Re^n \f$ denoted as \c Gf.  The NLP interface <tt>\ref NLPInterfacePack::NLPObjGradient "NLPObjGradient"</tt>
is defined to compute this gradient vector.

Gradient information for the constraints \a c(x) and \a h(x) can be harder to come by in many applications.  The NLP interface
<tt>\ref NLPInterfacePack::NLPFirstOrderInfo "NLPFirstOrderInfo"</tt> allows clients to create matrix objects that represent
the gradients of the constriants \f$ \nabla c(x) \in \Re^{n \times m} \f$ and \f$ \nabla h(x) \in \Re^{n \times mI} \f$ denoted
as \c Gc and \c Gh respectively.  These matrix objects are hidden behind the abstract interface
<tt>\ref AbstractLinAlgPack::MatrixWithOp "MatrixWithOp"</tt> that allows for different data
structure implementations and even parallelism.  Of course for these matrix objects to be of use to an optimization algorithm, 
more information is needed about them (i.e. see <tt>\ref AbstractLinAlgPack::BasisSystem "BasisSystem"</tt>) but this is
beyond the scope of these NLP interfaces.  

In some application areas, it is not even possible to form <tt>\ref AbstractLinAlgPack::MatrixWithOp "MatrixWithOp"</tt>
objects for \c Gc and \c Gh.  However, it may be possible to solve for some specific linear systems with sub-Jacobians
formed from these matrices.  The solutions from these specific linear systems are all that is needed by some gradient
based optimization algorithms (i.e. Reduced Space SQP).  For these specialized application areas, the NLP interface
<tt>\ref NLPInterfacePack::NLPFirstOrderDirect "NLPFirstOrderDirect"</tt> is defined.

Finally, perhaps the most difficult type of derivative information to obtain is second derivates in the form of 
symmetric Hessian matrices.  The use of such derivative information can make many optimization algorithms faster
and more robust.  For application areas that can support it, the NLP interface
<tt>\ref NLPInterfacePack::NLPSecondOrderInfo "NLPSecondOrderInfo"</tt> is defined.  This interface is capable of
computing a linear combination of the Hessians of the objective and constaint functions known as the Hessian of
the Lagrangian function (see <tt>\ref NLPInterfacePack::NLP "NLP"</tt> for a definition of the Lagrangian function
for this NLP formulation).  Again, this Hessian matrix object is abstracted behind a matrix interface, in this case
<tt>\ref AbstractLinAlgPack::MatrixSymWithOp "MatrixSymWithOp"</tt>, that allows for artibrary implementations
(even using parallelism).

*/

/** \defgroup libNLPInterfacePackTools_grp libNLPInterfacePackTools: Auxiliary tools for building NLP and testing NLP subclasses

This library contains a number of classes and subclasses that are useful in developing and testing concrete NLP implementations.

It is very common to approximate derivatives using finite differences.  In general, finite differencing can lead to inaccurate
derivatives when fine precision is needed and can be expensive to compute.  However, using finite differences to provide some
validation for derivatives computed by an NLP interface is invaluable.  The class
<tt>\ref NLPInterfacePack::CalcFiniteDiffProd "CalcFiniteDiffProd"</tt> can be used to
approximate the products <tt>Gf'*y</tt>, <tt>Gc'*y</tt> and <tt>Gh'*y</tt> for arbitrary vectors using one of several different
order methods.  Note that computing each of these products only requires between one to four extra function evaluations
per function.  Therefore, these products can be computed for even the largest problems.  The class
<tt>\ref NLPInterfacePack::CalcFiniteDiffFirstDerivatives "CalcFiniteDiffFirstDerivatives"</tt> can be used to compute
dense approximations for \c Gf, \c Gc and \c Gh.  In this case, computing these quantities will be prohibitively expensive
for larger problems and is not generally practical to do so.  However, computing these compete gradients by finite differences
can be very valuable to debug small NLPs.  Test finite differencing classes are used by several of the following testing
classes.

The above finite differencing driver classes can be useful in their own right but they are perhaps the most usefully in
developing automated testing software for NLP interfaces.  There are several NLP testing classes for various NLP interfaces.

The testing class <tt>\ref NLPInterfacePack::NLPTester "NLPTester"</tt> performs unit testing for the base
<tt>\ref NLPInterfacePack::NLP "NLP"</tt> interface.  No derivatives are checked here but must of the postconditions of all
of the methods are checked.  This class has several options that a user can select to suite the tests and the output to the
task at had (see the class <tt>\ref NLPInterfacePack::NLPTesterSetOptions "NLPTesterSetOptions"</tt>).

The testing class <tt>\ref NLPInterfacePack::NLPFirstOrderDirectTester "NLPFirstOrderDirectTester"</tt> tests the sensitivity
information computed by the <tt>\ref NLPInterfacePack::NLPFirstOrderDirect "NLPFirstOrderDirect"</tt> interface using finite
differences.  There are several options that control the behavior of this testing class and they can be set but the user from
and options file using <tt>\ref NLPInterfacePack::NLPFirstOrderDirectTesterSetOptions "NLPFirstOrderDirectTesterSetOptions"</tt>.

The testing function <tt>\ref NLPInterfacePack::test_nlp_first_order_direct "test_nlp_first_order_direct()"</tt> puts together
some testing components to perform some though tests of a <tt>\ref NLPInterfacePack::NLPFirstOrderDirect "NLPFirstOrderDirect"</tt>
object.

ToDo: Document testing code for NLPFirstOrderInfo when this code is ready.

*/

//@}
